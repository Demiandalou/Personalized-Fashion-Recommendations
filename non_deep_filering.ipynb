{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd \nimport time\nimport gc\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\nfrom multiprocessing import Pool\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom skimage import io\nfrom sklearn.decomposition import PCA\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:04:18.003742Z","iopub.execute_input":"2022-04-27T04:04:18.004022Z","iopub.status.idle":"2022-04-27T04:04:19.825442Z","shell.execute_reply.started":"2022-04-27T04:04:18.003994Z","shell.execute_reply":"2022-04-27T04:04:19.824417Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Method","metadata":{}},{"cell_type":"code","source":"# Source: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\ndef apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the average prescision at k between two lists of\n    items.\n    Parameters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=12):\n    \"\"\"\n    Computes the mean average precision at k.\n    This function computes the mean average prescision at k between two lists\n    of lists of items.\n    Parameters\n    ----------\n    actual : list\n             A list of lists of elements that are to be predicted \n             (order doesn't matter in the lists)\n    predicted : list\n                A list of lists of predicted elements\n                (order matters in the lists)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The mean average precision at k over the input lists\n    \"\"\"\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted) if a]) # CHANGES: ignore null actual (variable=a)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:18:26.310755Z","iopub.execute_input":"2022-04-27T04:18:26.312194Z","iopub.status.idle":"2022-04-27T04:18:26.322939Z","shell.execute_reply.started":"2022-04-27T04:18:26.312150Z","shell.execute_reply":"2022-04-27T04:18:26.321760Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# UUCF","metadata":{}},{"cell_type":"markdown","source":"**Data Processing**","metadata":{}},{"cell_type":"code","source":"base_path = '../input/h-and-m-personalized-fashion-recommendations/'\ncsv_train = f'{base_path}transactions_train.csv'\ncsv_users = f'{base_path}customers.csv'\ncsv_items = f'{base_path}articles.csv'","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-27T04:07:57.489282Z","iopub.execute_input":"2022-04-27T04:07:57.489760Z","iopub.status.idle":"2022-04-27T04:07:57.493807Z","shell.execute_reply.started":"2022-04-27T04:07:57.489726Z","shell.execute_reply":"2022-04-27T04:07:57.493168Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"transactions = pd.read_csv(csv_train, dtype={'article_id': str}, parse_dates=['t_dat'])\nusers = pd.read_csv(csv_users)\nitems = pd.read_csv(csv_items, dtype={'article_id': str})","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-27T04:07:59.509795Z","iopub.execute_input":"2022-04-27T04:07:59.510451Z","iopub.status.idle":"2022-04-27T04:09:30.956162Z","shell.execute_reply.started":"2022-04-27T04:07:59.510406Z","shell.execute_reply":"2022-04-27T04:09:30.955386Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"1. **Map user_id in users to cutomer_id in transactions**\n2. **Map item_id in users to article_id in transactions**","metadata":{"editable":false}},{"cell_type":"code","source":"user_list = users['customer_id'].unique().tolist()\nitem_list = items['article_id'].unique().tolist()\n\nuser_to_customer_map = {user_id: customer_id for user_id, customer_id in enumerate(user_list)}\ncustomer_to_user_map = {customer_id: user_id for user_id, customer_id in enumerate(user_list)}\n\nitem_to_article_map = {item_id: article_id for item_id, article_id in enumerate(item_list)}\narticle_to_item_map = {article_id: item_id for item_id, article_id in enumerate(item_list)}","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-27T04:09:33.716954Z","iopub.execute_input":"2022-04-27T04:09:33.717224Z","iopub.status.idle":"2022-04-27T04:09:35.102039Z","shell.execute_reply.started":"2022-04-27T04:09:33.717192Z","shell.execute_reply":"2022-04-27T04:09:35.100851Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"transactions['user_id'] = transactions['customer_id'].map(customer_to_user_map)\ntransactions['item_id'] = transactions['article_id'].map(article_to_item_map)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2022-04-27T04:09:35.103062Z","iopub.execute_input":"2022-04-27T04:09:35.103292Z","iopub.status.idle":"2022-04-27T04:09:52.958898Z","shell.execute_reply.started":"2022-04-27T04:09:35.103266Z","shell.execute_reply":"2022-04-27T04:09:52.957750Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"val_start_date = '2020-09-16'\ntrain_df = transactions.query(f\"t_dat < '{val_start_date}'\").reset_index(drop=True)\nvalid_df = transactions.query(f\"t_dat >= '{val_start_date}'\").reset_index(drop=True)\n\n# Sorting\ntrain_df = train_df.sort_values([\"customer_id\", \"t_dat\"], ascending=False)\nvalid_df = valid_df.sort_values([\"customer_id\", \"t_dat\"], ascending=False)\n\n_ = gc.collect()\n\nvalid_df = valid_df.sort_values(['customer_id', 't_dat'], ascending = [True, True]) \nvalid_cust = valid_df.groupby('customer_id')['article_id'].apply(list).reset_index()\nvalid_cust['valid_true'] = valid_cust['article_id']\ndel valid_df, valid_cust['article_id']\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:09:52.960368Z","iopub.execute_input":"2022-04-27T04:09:52.960664Z","iopub.status.idle":"2022-04-27T04:10:34.397298Z","shell.execute_reply.started":"2022-04-27T04:09:52.960624Z","shell.execute_reply":"2022-04-27T04:10:34.395831Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**Helper Method**","metadata":{"editable":false}},{"cell_type":"code","source":"TEST_RUN = True\nTEST_SIZE = 10000\n\ndef flatten(l):\n    return [item for sublist in l for item in sublist]\n\ndef compare_user(user_1, user_2):\n    return len(set(user_1) & set(user_2)) / np.sqrt(len(user_1) * len(user_2))\n\ndef recommend_item(user_id, user_vec, transaction_history):\n    global n\n    similar_users = transaction_history.apply(lambda other_vec: compare_user(user_vec, other_vec)).sort_values(ascending=False).head(31)\n    \n    similar_users = similar_users[similar_users.index != user_id]\n        \n    users, scores = similar_users.index.tolist(), similar_users.tolist()\n    \n    user_score = pd.DataFrame({'user': users, 'score': scores})\n    user_score['items'] = user_score.apply(lambda row: transaction_history.loc[row.user], axis=1)\n    user_score['weighted_items'] = user_score.apply(lambda row: [(item, row.score) for item in row['items']], axis=1)\n\n    recs = pd.DataFrame(flatten(user_score['weighted_items'].tolist()), columns=['item', 'score']).groupby('item')['score'].sum().sort_values(ascending=False)\n    recs = recs[~recs.index.isin(user_vec)]\n    # Keep the first 12 and get the item_ids\n    return recs.head(12).index.tolist()\n\ndef recommend_list_user(user_ids, transaction_history):\n    global n    \n    n = len(user_ids)    \n    user_vectors = pd.DataFrame(transaction_history.loc[user_ids]).reset_index()\n    user_vectors['item_id'] = user_vectors.apply(lambda row: recommend_item(row.user_id, row.item_id, transaction_history), axis=1)\n    return user_vectors.set_index('user_id')['item_id']\n\ndef get_recommendations(users, transaction_history):\n    time_start = time.time()\n    \n    # Split into approximately evenly sized chunks\n    # We will send just one batch to each CPU \n    user_chunks = np.array_split(users, mp.cpu_count())\n    \n    f = partial(recommend_list_user, transaction_history=transaction_history)\n    with Pool(mp.cpu_count()) as p:\n        res = p.map(f, user_chunks)\n    \n    recommendation = pd.DataFrame(pd.concat(res))\n\n    elapsed = (time.time() - time_start) / 60\n    print(f\"Finished get_recommendations({len(users)}). It took {elapsed:5.2f} mins\")\n    return recommendation\n\ndef uucf(transactions, start_date):\n    transaction_history = transactions\n    print(f\"Kept data from {start_date} on. Total rows: {len(transaction_history)}\")\n    \n    transaction_history = transaction_history.groupby(\"user_id\")['item_id'].apply(lambda items: list(set(items)))\n    transaction_history = transaction_history[transaction_history.str.len() >= 3]\n    if TEST_RUN:\n        print(\"WARNING: TEST_RUN is True. It will be a toy execution.\")\n        transaction_history = transaction_history.head(TEST_SIZE)\n    \n    users = transaction_history.index.tolist()\n    n_users = len(users)\n    print(f\"Total users in the time frame with at least 3 purchases: {n_users}\")\n    \n    recommendation = get_recommendations(users, transaction_history)\n    recommendation['customer_id'] = recommendation.index.map(user_to_customer_map)\n    recommendation['prediction'] = recommendation['item_id'].map(lambda l: [item_to_article_map[i] for i in l])\n    recommendation.reset_index(drop=True)[['customer_id', 'prediction']]\n    return recommendation \n\ndef uu_plot_prev(index):\n    prev_items = list(set(eval_uucf.iloc[index][\"valid_true\"]))\n    path = \"../input/h-and-m-personalized-fashion-recommendations/images\"\n    fig = plt.figure(figsize=(20, 6))\n    plt.title(\"Purchased items\")\n    plt.axis(\"off\")\n    plt.xticks(())\n    plt.yticks(())\n\n    for item, i in zip(prev_items, range(1, len(prev_items)+1)):\n        sub = item[:3]\n        image = path + \"/\"+ sub + \"/\"+ item +\".jpg\"\n        image = plt.imread(image)\n        fig.add_subplot(1, len(prev_items), i)\n        plt.xticks(())\n        plt.yticks(())\n        plt.imshow(image)\ndef uu_plot_rcmd(index):\n    rcmd_items = list(set(eval_uucf.iloc[index][\"prediction\"]))\n    path = \"../input/h-and-m-personalized-fashion-recommendations/images\"\n    fig = plt.figure(figsize=(20, 6))\n    plt.title(\"Recommended items\")\n    plt.axis(\"off\")\n\n    for item, i in zip(rcmd_items, range(1, len(rcmd_items)+1)):\n        sub = item[:3]\n        image = path + \"/\"+ sub + \"/\"+ item +\".jpg\"\n        image = plt.imread(image)\n        fig.add_subplot(1, len(rcmd_items), i)\n        plt.xticks(())\n        plt.yticks(())\n        plt.imshow(image)\n    \n    \ndef uu_plot(index):\n    uu_plot_prev(index)\n    uu_plot_rcmd(index)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:10:34.401073Z","iopub.execute_input":"2022-04-27T04:10:34.401355Z","iopub.status.idle":"2022-04-27T04:10:34.433155Z","shell.execute_reply.started":"2022-04-27T04:10:34.401321Z","shell.execute_reply":"2022-04-27T04:10:34.431719Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Train Model**","metadata":{}},{"cell_type":"code","source":"recommendation = uucf(train_df,val_start_date)\neval_uucf = recommendation\neval_uucf = valid_cust.merge(eval_uucf, on ='customer_id', how ='left')\neval_uucf = eval_uucf.dropna(axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:10:34.434527Z","iopub.execute_input":"2022-04-27T04:10:34.434809Z","iopub.status.idle":"2022-04-27T04:17:11.546914Z","shell.execute_reply.started":"2022-04-27T04:10:34.434778Z","shell.execute_reply":"2022-04-27T04:17:11.545267Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation**","metadata":{}},{"cell_type":"code","source":"mapk(\n    eval_uucf['valid_true'], \n    eval_uucf['prediction'],\n    k=12\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:18:47.063185Z","iopub.execute_input":"2022-04-27T04:18:47.064076Z","iopub.status.idle":"2022-04-27T04:18:47.078141Z","shell.execute_reply.started":"2022-04-27T04:18:47.064034Z","shell.execute_reply":"2022-04-27T04:18:47.077366Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"****","metadata":{}},{"cell_type":"code","source":"uu_plot(4)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:18:49.970926Z","iopub.execute_input":"2022-04-27T04:18:49.971816Z","iopub.status.idle":"2022-04-27T04:18:56.427861Z","shell.execute_reply.started":"2022-04-27T04:18:49.971758Z","shell.execute_reply":"2022-04-27T04:18:56.426498Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Content-based Filtering with PCA","metadata":{}},{"cell_type":"markdown","source":"**Data Processing for CBF**","metadata":{}},{"cell_type":"code","source":"p_df = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv', chunksize=100000)\narticles = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv')\np_users = next(p_df)\np_df = p_users.merge(articles, on='article_id')\ndf1 = p_df[['customer_id', 'article_id',\n       'product_group_name', \n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name',\n       'perceived_colour_master_name',\n       'department_name', 'index_name',\n       'index_group_name', 'section_name',\n       'garment_group_name']]\nfeatures = ['product_group_name', \n       'graphical_appearance_name', 'colour_group_name',\n       'perceived_colour_value_name',\n       'perceived_colour_master_name',\n       'department_name', 'index_name',\n       'index_group_name', 'section_name',\n       'garment_group_name']\n\none_hot_df = pd.get_dummies(df1, columns=features) # one hot encoding\n\n# Storing item and user information in one_hot df\nMIN_PURCHASE = 2\ngroupby_customer = one_hot_df.groupby('customer_id')\n\n# l stores all items bought for each user represented using one hot encoding\nl = []\ncutomer_ids = []\narticle_ids = []\nfor key in groupby_customer.groups.keys():\n    temp = groupby_customer.get_group(key)\n    if temp.article_id.nunique() >= MIN_PURCHASE:\n        l.append(temp.drop('article_id', axis=1).sum(numeric_only=True).values)\n        cutomer_ids.append(key)\n        article_ids.extend(temp.article_id.values.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:04:32.162973Z","iopub.execute_input":"2022-04-27T04:04:32.164130Z","iopub.status.idle":"2022-04-27T04:05:33.458702Z","shell.execute_reply.started":"2022-04-27T04:04:32.164087Z","shell.execute_reply":"2022-04-27T04:05:33.457639Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"user_feature = pd.DataFrame(l, columns = one_hot_df.columns[2:])\nnormalized_user_feature = user_feature.div(user_feature.sum(axis=1), axis=0)\nnormalized_user_feature.insert(0, 'customer_id', cutomer_ids)\nnormalized_user_feature = normalized_user_feature.set_index('customer_id')\npca = PCA(n_components=100)\npca.fit(normalized_user_feature)\npca.explained_variance_ratio_.sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:05:52.143149Z","iopub.execute_input":"2022-04-27T04:05:52.143531Z","iopub.status.idle":"2022-04-27T04:06:02.757086Z","shell.execute_reply.started":"2022-04-27T04:05:52.143439Z","shell.execute_reply":"2022-04-27T04:06:02.756094Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# drop duplicates and items boughts already\nitem_feature = one_hot_df.drop_duplicates(subset='article_id')\nitem_feature = item_feature[item_feature.article_id.isin(article_ids)].drop('customer_id', axis=1)\nitem_feature = item_feature.set_index('article_id')\nuser_feature_pca = pd.DataFrame(pca.transform(normalized_user_feature), columns=['component_{}'.format(i) for i in range(1, 101)]).set_index(normalized_user_feature.index)\nitem_feature_pca = pd.DataFrame(pca.transform(item_feature), columns=['component_{}'.format(i) for i in range(1, 101)]).set_index(item_feature.index)\nscores_pca = user_feature_pca.dot(item_feature_pca.T)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:06:09.274940Z","iopub.execute_input":"2022-04-27T04:06:09.275395Z","iopub.status.idle":"2022-04-27T04:06:11.874131Z","shell.execute_reply.started":"2022-04-27T04:06:09.275362Z","shell.execute_reply":"2022-04-27T04:06:11.873114Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Helper method**","metadata":{}},{"cell_type":"code","source":"def get_rcmnd(customer_id, scores):\n    cutomer_scores = scores.loc[customer_id]\n    customer_prev_items = groupby_customer.get_group(customer_id)['article_id']\n    prev_dropped = cutomer_scores.drop(customer_prev_items.values)\n    ordered = prev_dropped.sort_values(ascending=False)   \n    return ordered, customer_prev_items\n\n\ndef cbf_plot_prev(prev):\n    path = \"../input/h-and-m-personalized-fashion-recommendations/images\"\n    fig = plt.figure(figsize=(20, 5))\n    plt.title(\"Purchased items\")\n    plt.axis(\"off\")    \n    for item, i in zip(prev, range(1, len(prev)+1)):\n        item = '0' + str(item)\n        sub = item[:3]\n        image = path + \"/\"+ sub + \"/\"+ item +\".jpg\"\n        image = plt.imread(image)\n        fig.add_subplot(1, len(prev), i)\n        plt.xticks(())\n        plt.yticks(())\n        plt.imshow(image)\n        \ndef cbf_plot_rcmnd(rcmnds):\n    path = \"../input/h-and-m-personalized-fashion-recommendations/images\"\n    fig = plt.figure(figsize=(20, 5))\n    plt.title(\"Recommended items\")\n    plt.axis(\"off\")\n    for item, i in zip(rcmnds, range(1, len(rcmnds)+1)):\n        item = '0' + str(item)\n        sub = item[:3]\n        image = path + \"/\"+ sub + \"/\"+ item +\".jpg\"\n        image = plt.imread(image)\n        fig.add_subplot(1, len(rcmnds), i)\n        plt.xticks(())\n        plt.yticks(())\n        plt.imshow(image)\n                        \ndef cbf_plot(index):\n    customer_id = scores.index[index]\n    rcmnds_pca, prev_items = get_rcmnd(customer_id, scores_pca)\n    rcmnds_pca = rcmnds_pca.index.values[:12]\n    cbf_plot_prev(prev_items)\n    cbf_plot_rcmnd(rcmnds_pca)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:19:18.753980Z","iopub.execute_input":"2022-04-27T04:19:18.754870Z","iopub.status.idle":"2022-04-27T04:19:18.768312Z","shell.execute_reply.started":"2022-04-27T04:19:18.754834Z","shell.execute_reply":"2022-04-27T04:19:18.767647Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"**Visualization**","metadata":{}},{"cell_type":"code","source":"cbf_plot(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T04:19:39.388015Z","iopub.execute_input":"2022-04-27T04:19:39.388421Z","iopub.status.idle":"2022-04-27T04:19:48.366918Z","shell.execute_reply.started":"2022-04-27T04:19:39.388381Z","shell.execute_reply":"2022-04-27T04:19:48.365908Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation**","metadata":{}},{"cell_type":"code","source":"mapk(\n    eval_cbf['valid_true'], \n    eval_cbf['prediction'],\n    k=12\n)","metadata":{},"execution_count":null,"outputs":[]}]}